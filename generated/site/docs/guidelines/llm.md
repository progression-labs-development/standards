
## LLM Services

Use the `chrismlittle123/llm` package for all LLM functionality.

### Requirements

- Use `chrismlittle123/llm` for all LLM calls — never integrate providers directly
- Use `chrismlittle123/llm` for all LLM observability — never integrate Langfuse directly
- Use `chrismlittle123/llm` for RAG and evaluations

### Installation

```bash
uv add chrismlittle123/llm
```

### What the Package Provides

| Feature | Description |
|---------|-------------|
| Unified API | Single interface for OpenAI, Anthropic, Google |
| Observability | Tracing, token counts, latency, cost via Langfuse |
| RAG | Retrieval-augmented generation utilities |
| Evals | DeepEval wrapper for testing LLM outputs |
| Fallbacks | Primary model fails → backup |
| Cost tracking | Per project, feature, user |

### Required Metadata

All LLM calls must include:

| Field | Description |
|-------|-------------|
| `project` | Project identifier |
| `feature` | Feature or use case name |
| `userId` | User who triggered the call |
| `requestId` | Correlation ID (links to app logs) |

### What Gets Tracked

- Model used
- Token count (input/output)
- Latency
- Cost
- Success/failure
- Prompt and response content

### Correlation with App Logs

Use the same `requestId` in both `chrismlittle123/monitoring` and `chrismlittle123/llm` to correlate:
- SigNoz error → Langfuse LLM trace
- LLM latency spike → App request that triggered it

### What NOT to Do

- Import OpenAI/Anthropic/Google SDKs directly
- Integrate Langfuse directly
- Skip metadata on LLM calls
- Use different correlation IDs between app and LLM logs

Refer to [chrismlittle123/llm](https://github.com/chrismlittle123/llm) for full documentation.